{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import random\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from time import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetlist = ['InVS13','InVS15','LH10','LyonSchool','SFHH','Thiers13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_networks(data_dir, dataset, n_minutes=5, original_nets=True):\n",
    "    \"\"\"Function that reads the edgelist (t, i, j) and returns\n",
    "    a network aggregated at n_minutes snapshots as a dictionary of nx.Graph()s,\n",
    "    having t as a key.\n",
    "    If original_nets is set to True it also returns the original non-aggregated network.\"\"\"\n",
    "    \n",
    "    #Reading the data and setting t0\n",
    "    f = open(data_dir+'/tij_' + dataset +'.dat')\n",
    "    (t0,i,j) = map(int,str.split(f.readline()))\n",
    "    #Special temporal scale for these two Datasets\n",
    "    if dataset not in ['LyonSchool','LH10']:\n",
    "        t0 = t0*20\n",
    "    f.close()\n",
    "    \n",
    "    #Aggregation on scale of x minutes\n",
    "    delta_t = 20*3*n_minutes;   \n",
    "    if original_nets==True:\n",
    "        originalnetworks = {}\n",
    "    aggnetworks = {}\n",
    "    f = open(data_dir+'/tij_' + dataset +'.dat')\n",
    "    for line in f:\n",
    "        (t,i,j) = map(int,str.split(line))\n",
    "        #Special temporal scale for these two Datasets\n",
    "        if dataset not in ['LyonSchool','LH10']:\n",
    "            t = t*20\n",
    "        if original_nets==True:\n",
    "            if t not in originalnetworks:\n",
    "                originalnetworks[t] = nx.Graph()\n",
    "            originalnetworks[t].add_edge(i,j)\n",
    "        #this is a trick using the integer division in python\n",
    "        aggtime = t0 + ((t-t0)/delta_t)*delta_t \n",
    "        if aggtime not in aggnetworks:\n",
    "            aggnetworks[aggtime] = nx.Graph()\n",
    "        aggnetworks[aggtime].add_edge(i,j)\n",
    "    f.close();\n",
    "    if original_nets==True:\n",
    "        return originalnetworks, aggnetworks;\n",
    "    else:\n",
    "        return aggnetworks;\n",
    "    \n",
    "def extract_cliques(gs):\n",
    "    listsaggcliques = {}\n",
    "    #looping over the networks in temporal order\n",
    "    for t in sorted(gs.keys()):\n",
    "        listsaggcliques[t] = list(nx.find_cliques(gs[t]));\n",
    "    #returning a dictionary with list of cliques as values\n",
    "    return listsaggcliques;\n",
    "    \n",
    "def clique_weights(cliques):\n",
    "    from collections import Counter;\n",
    "    tot_c = [];\n",
    "    for t in cliques:\n",
    "        tot_c.extend(map(frozenset,cliques[t]))\n",
    "    return Counter(tot_c);\n",
    "\n",
    "def average_clique_size(ws):\n",
    "    return np.sum(map(lambda x: 1.0 * ws[x] * len(x), ws.keys()))/np.sum(ws.values());\n",
    "\n",
    "def clean_non_maximal(ws):\n",
    "    sd = dict(zip(ws.keys(), map(len,ws.keys())));\n",
    "    import operator\n",
    "    sizes = set(map(len,ws.keys()));\n",
    "    sorted_sd = sorted(sd.items(), key=operator.itemgetter(1));\n",
    "    simplices = dict.fromkeys(list(sizes),[]);\n",
    "    maximal_simplices = {};\n",
    "    for x in ws:\n",
    "        maximal = True;\n",
    "        for xx in ws:\n",
    "            if (len(x)<len(xx)):\n",
    "                if (set(x)<set(xx)):\n",
    "                    maximal=False;\n",
    "                    break;\n",
    "        if maximal:\n",
    "            maximal_simplices[x] = ws[x];\n",
    "    return maximal_simplices;\n",
    "\n",
    "def save_cliques(ws, data_dir, dataset,n_minutes, thr=None):\n",
    "    if thr==None:\n",
    "        ls = map(list,ws.keys());\n",
    "    else:\n",
    "        ls = [list(x) for x in ws if ws[x]>=thr];\n",
    "    jd = open(data_dir+'aggr_'+str(n_minutes)+'min_cliques_thr'+str(thr)+'_'+dataset+'.json','w')\n",
    "    json.dump(ls,jd)\n",
    "    jd.close()\n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '../Data/Sociopatterns/Originaldata_20s/' \n",
    "out_dir = '../Data/Sociopatterns/Processed_data/' \n",
    "\n",
    "datasets = ['InVS13','InVS15','LH10','LyonSchool','SFHH','Thiers13']\n",
    "\n",
    "n_minutes = 15\n",
    "thrs = [1,3,5]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for thr in thrs:\n",
    "        aggs = extract_networks(dataset_dir, dataset, n_minutes, original_nets=False);\n",
    "        cliques = extract_cliques(aggs)\n",
    "        ws = clique_weights(cliques);\n",
    "        maximal_cliques = clean_non_maximal(ws);\n",
    "        save_cliques(maximal_cliques, out_dir, dataset, n_minutes, thr=thr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
